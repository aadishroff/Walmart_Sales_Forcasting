{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3638db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1033278d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd37eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbc35c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34e8b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"myApp\").config(\"spark.mongodb.input.uri\",\"mongodb://localhost:27017/project.walmart?readPreference=primaryPreferred\").config(\"spark.mongodb.output.uri\",\"mongodb://localhost:27017/project.walmart\").config(\"spark.jars.packages\",\"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cda4bee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/features.csv\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"features.csv\")\n",
    "features=spark.read.csv(\"file:///\"+SparkFiles.get(\"features.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cb83a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/stores.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"stores.csv\")\n",
    "stores=spark.read.csv(\"file:///\"+SparkFiles.get(\"stores.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1e37c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/train.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"train.csv\")\n",
    "train=spark.read.csv(\"file:///\"+SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "667221e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://raw.githubusercontent.com/kirtigupta10007/Walmart-Store-Sales-Forecasting/master/data/test.csv\"\n",
    "\n",
    "spark.sparkContext.addFile(url)\n",
    "SparkFiles.get(\"test.csv\")\n",
    "test=spark.read.csv(\"file:///\"+SparkFiles.get(\"test.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cfdf2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|Store|               Date|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|IsHoliday|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "|    1|2010-02-05 00:00:00|      42.31|     2.572|       NA|       NA|       NA|       NA|       NA|211.0963582|       8.106|    false|\n",
      "|    1|2010-02-12 00:00:00|      38.51|     2.548|       NA|       NA|       NA|       NA|       NA|211.2421698|       8.106|     true|\n",
      "|    1|2010-02-19 00:00:00|      39.93|     2.514|       NA|       NA|       NA|       NA|       NA|211.2891429|       8.106|    false|\n",
      "|    1|2010-02-26 00:00:00|      46.63|     2.561|       NA|       NA|       NA|       NA|       NA|211.3196429|       8.106|    false|\n",
      "|    1|2010-03-05 00:00:00|       46.5|     2.625|       NA|       NA|       NA|       NA|       NA|211.3501429|       8.106|    false|\n",
      "+-----+-------------------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9433e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "|Store|Type|  Size|\n",
      "+-----+----+------+\n",
      "|    1|   A|151315|\n",
      "|    2|   A|202307|\n",
      "|    3|   B| 37392|\n",
      "|    4|   A|205863|\n",
      "|    5|   B| 34875|\n",
      "+-----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stores.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ea45cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+------------+---------+\n",
      "|Store|Dept|               Date|Weekly_Sales|IsHoliday|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "|    1|   1|2010-02-05 00:00:00|     24924.5|    false|\n",
      "|    1|   1|2010-02-12 00:00:00|    46039.49|     true|\n",
      "|    1|   1|2010-02-19 00:00:00|    41595.55|    false|\n",
      "|    1|   1|2010-02-26 00:00:00|    19403.54|    false|\n",
      "|    1|   1|2010-03-05 00:00:00|     21827.9|    false|\n",
      "+-----+----+-------------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "935de44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------------------+---------+\n",
      "|Store|Dept|               Date|IsHoliday|\n",
      "+-----+----+-------------------+---------+\n",
      "|    1|   1|2012-11-02 00:00:00|    false|\n",
      "|    1|   1|2012-11-09 00:00:00|    false|\n",
      "|    1|   1|2012-11-16 00:00:00|    false|\n",
      "|    1|   1|2012-11-23 00:00:00|     true|\n",
      "|    1|   1|2012-11-30 00:00:00|    false|\n",
      "+-----+----+-------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30944894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Date': 0, 'Temperature': 0, 'Fuel_Price': 0, 'MarkDown1': 0, 'MarkDown2': 0, 'MarkDown3': 0, 'MarkDown4': 0, 'MarkDown5': 0, 'CPI': 0, 'Unemployment': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:features.filter(features[col].isNull()).count() for col in features.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c17dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Type': 0, 'Size': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:stores.filter(stores[col].isNull()).count() for col in stores.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04524517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Dept': 0, 'Date': 0, 'Weekly_Sales': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:train.filter(train[col].isNull()).count() for col in train.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d032f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Store': 0, 'Dept': 0, 'Date': 0, 'IsHoliday': 0}\n"
     ]
    }
   ],
   "source": [
    "chcekNullValues = {col:test.filter(test[col].isNull()).count() for col in test.columns}\n",
    "print(chcekNullValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c4bfbf",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73e2482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux=train.join(stores, [\"Store\"])\n",
    "\n",
    "\n",
    "df_tests=test.join(stores,[\"Store\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "822c3ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_aux.join(features, on=['Store', 'Date','IsHoliday'], how='inner')\n",
    "\n",
    "\n",
    "df_test = df_tests.join(features, on=['Store', 'Date','IsHoliday'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c520789c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: string (nullable = true)\n",
      " |-- MarkDown2: string (nullable = true)\n",
      " |-- MarkDown3: string (nullable = true)\n",
      " |-- MarkDown4: string (nullable = true)\n",
      " |-- MarkDown5: string (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n",
      "*******************\n",
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: string (nullable = true)\n",
      " |-- MarkDown2: string (nullable = true)\n",
      " |-- MarkDown3: string (nullable = true)\n",
      " |-- MarkDown4: string (nullable = true)\n",
      " |-- MarkDown5: string (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()\n",
    "print(\"*******************\")\n",
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "322a0b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Type|  Size|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|\n",
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|    1|2010-02-05 00:00:00|    false|   1|     24924.5|   A|151315|      42.31|     2.572|        0|        0|        0|        0|        0|211.0963582|       8.106|\n",
      "|    1|2010-02-12 00:00:00|     true|   1|    46039.49|   A|151315|      38.51|     2.548|        0|        0|        0|        0|        0|211.2421698|       8.106|\n",
      "|    1|2010-02-19 00:00:00|    false|   1|    41595.55|   A|151315|      39.93|     2.514|        0|        0|        0|        0|        0|211.2891429|       8.106|\n",
      "|    1|2010-02-26 00:00:00|    false|   1|    19403.54|   A|151315|      46.63|     2.561|        0|        0|        0|        0|        0|211.3196429|       8.106|\n",
      "|    1|2010-03-05 00:00:00|    false|   1|     21827.9|   A|151315|       46.5|     2.625|        0|        0|        0|        0|        0|211.3501429|       8.106|\n",
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: integer (nullable = true)\n",
      " |-- MarkDown2: integer (nullable = true)\n",
      " |-- MarkDown3: integer (nullable = true)\n",
      " |-- MarkDown4: integer (nullable = true)\n",
      " |-- MarkDown5: integer (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df3 = df_train.withColumn(\"MarkDown1\", when(df_train.MarkDown1 == \"NA\",0))\n",
    "df3 = df3.withColumn(\"MarkDown2\", when(df3.MarkDown2 == \"NA\",0)) \n",
    "df3 = df3.withColumn(\"MarkDown3\", when(df3.MarkDown3 == \"NA\",0))\n",
    "df3 = df3.withColumn(\"MarkDown4\", when(df3.MarkDown4 == \"NA\",0))\n",
    "df_train = df3.withColumn(\"MarkDown5\", when(df3.MarkDown5 == \"NA\",0))\n",
    "                          \n",
    "\n",
    "df_train.show(5)\n",
    "df_train.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "172b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_train.withColumn('MarkDown1',df_train.MarkDown1.cast(\"float\"))\n",
    "df_train=df_train.withColumn('MarkDown2',df_train.MarkDown2.cast(\"float\"))\n",
    "df_train=df_train.withColumn('MarkDown3',df_train.MarkDown3.cast(\"float\"))\n",
    "df_train=df_train.withColumn('MarkDown4',df_train.MarkDown4.cast(\"float\"))\n",
    "train=df_train.withColumn('MarkDown5',df_train.MarkDown5.cast(\"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe2ad49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Weekly_Sales: double (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: float (nullable = true)\n",
      " |-- MarkDown2: float (nullable = true)\n",
      " |-- MarkDown3: float (nullable = true)\n",
      " |-- MarkDown4: float (nullable = true)\n",
      " |-- MarkDown5: float (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n",
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|Store|               Date|IsHoliday|Dept|Weekly_Sales|Type|  Size|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|\n",
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|    1|2010-02-05 00:00:00|    false|   1|     24924.5|   A|151315|      42.31|     2.572|      0.0|      0.0|      0.0|      0.0|      0.0|211.0963582|       8.106|\n",
      "|    1|2010-02-12 00:00:00|     true|   1|    46039.49|   A|151315|      38.51|     2.548|      0.0|      0.0|      0.0|      0.0|      0.0|211.2421698|       8.106|\n",
      "+-----+-------------------+---------+----+------------+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()\n",
    "train.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f202715b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|Store|               Date|IsHoliday|Dept|Type|  Size|Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|        CPI|Unemployment|\n",
      "+-----+-------------------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "|    1|2012-11-02 00:00:00|    false|   1|   A|151315|      55.32|     3.386|     null|     null|     null|     null|     null|223.4627793|       6.573|\n",
      "|    1|2012-11-09 00:00:00|    false|   1|   A|151315|      61.24|     3.314|     null|     null|     null|     null|     null|223.4813073|       6.573|\n",
      "|    1|2012-11-16 00:00:00|    false|   1|   A|151315|      52.92|     3.252|     null|     null|     null|     null|     null|223.5129105|       6.573|\n",
      "|    1|2012-11-23 00:00:00|     true|   1|   A|151315|      56.23|     3.211|     null|     null|     null|     null|     null|223.5619474|       6.573|\n",
      "|    1|2012-11-30 00:00:00|    false|   1|   A|151315|      52.34|     3.207|     null|        0|     null|     null|     null|223.6109842|       6.573|\n",
      "+-----+-------------------+---------+----+----+------+-----------+----------+---------+---------+---------+---------+---------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df_test.withColumn(\"MarkDown1\", when(df_test.MarkDown1 == \"NA\",0))\n",
    "df4 = df4.withColumn(\"MarkDown2\", when(df4.MarkDown2 == \"NA\",0)) \n",
    "df4 = df4.withColumn(\"MarkDown3\", when(df4.MarkDown3 == \"NA\",0))\n",
    "df4 = df4.withColumn(\"MarkDown4\", when(df4.MarkDown4 == \"NA\",0))\n",
    "df_test = df4.withColumn(\"MarkDown5\", when(df4.MarkDown5 == \"NA\",0))\n",
    "                          \n",
    "\n",
    "df_test.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4825fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Store: integer (nullable = true)\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- IsHoliday: boolean (nullable = true)\n",
      " |-- Dept: integer (nullable = true)\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Temperature: double (nullable = true)\n",
      " |-- Fuel_Price: double (nullable = true)\n",
      " |-- MarkDown1: float (nullable = true)\n",
      " |-- MarkDown2: float (nullable = true)\n",
      " |-- MarkDown3: float (nullable = true)\n",
      " |-- MarkDown4: float (nullable = true)\n",
      " |-- MarkDown5: float (nullable = true)\n",
      " |-- CPI: string (nullable = true)\n",
      " |-- Unemployment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test=df_test.withColumn('MarkDown1',df_test.MarkDown1.cast(\"float\"))\n",
    "df_test=df_test.withColumn('MarkDown2',df_test.MarkDown2.cast(\"float\"))\n",
    "df_test=df_test.withColumn('MarkDown3',df_test.MarkDown3.cast(\"float\"))\n",
    "df_test=df_test.withColumn('MarkDown4',df_test.MarkDown4.cast(\"float\"))\n",
    "test=df_test.withColumn('MarkDown5',df_test.MarkDown5.cast(\"float\"))\n",
    "test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fac73529",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o251.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 96.0 failed 1 times, most recent failure: Lost task 1.0 in stage 96.0 (TID 91) (DESKTOP-376FPEQ executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast 0.0 into a BsonValue. FloatType has no matching BsonValue.\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$15(MapFunctions.scala:122)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$16(MapFunctions.scala:125)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$2(MapFunctions.scala:82)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$1(MapFunctions.scala:82)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$3(MapFunctions.scala:62)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$5(MapFunctions.scala:73)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$4(MapFunctions.scala:70)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:154)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\r\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:117)\r\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:159)\r\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:78)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast 0.0 into a BsonValue. FloatType has no matching BsonValue.\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$15(MapFunctions.scala:122)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$16(MapFunctions.scala:125)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$2(MapFunctions.scala:82)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$1(MapFunctions.scala:82)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$3(MapFunctions.scala:62)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$5(MapFunctions.scala:73)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$4(MapFunctions.scala:70)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:154)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\r\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ml \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muri\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb://localhost:27017/walmart.data_train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muri\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmongodb://localhost:27017/walmart.data_train\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Project_cdac\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:966\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[1;32m~\\Desktop\\Project_cdac\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Desktop\\Project_cdac\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\Desktop\\Project_cdac\\spark-3.3.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o251.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 96.0 failed 1 times, most recent failure: Lost task 1.0 in stage 96.0 (TID 91) (DESKTOP-376FPEQ executor driver): com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast 0.0 into a BsonValue. FloatType has no matching BsonValue.\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$15(MapFunctions.scala:122)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$16(MapFunctions.scala:125)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$2(MapFunctions.scala:82)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$1(MapFunctions.scala:82)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$3(MapFunctions.scala:62)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$5(MapFunctions.scala:73)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$4(MapFunctions.scala:70)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:154)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\r\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1009)\r\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:117)\r\n\tat com.mongodb.spark.MongoSpark$.save(MongoSpark.scala:159)\r\n\tat com.mongodb.spark.sql.DefaultSource.createRelation(DefaultSource.scala:78)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\nCaused by: com.mongodb.spark.exceptions.MongoTypeConversionException: Cannot cast 0.0 into a BsonValue. FloatType has no matching BsonValue.\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$15(MapFunctions.scala:122)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$dataTypeToBsonValueMapper$16(MapFunctions.scala:125)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$2(MapFunctions.scala:82)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$wrappedDataTypeToBsonValueMapper$1(MapFunctions.scala:82)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$3(MapFunctions.scala:62)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$5(MapFunctions.scala:73)\r\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\r\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\r\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\r\n\tat com.mongodb.spark.sql.MapFunctions$.$anonfun$rowToDocumentMapper$4(MapFunctions.scala:70)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$4(MongoSpark.scala:154)\r\n\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1161)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2(MongoSpark.scala:119)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$2$adapted(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withCollectionDo$1(MongoConnector.scala:186)\r\n\tat com.mongodb.spark.MongoConnector.$anonfun$withDatabaseDo$1(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withMongoClientDo(MongoConnector.scala:154)\r\n\tat com.mongodb.spark.MongoConnector.withDatabaseDo(MongoConnector.scala:171)\r\n\tat com.mongodb.spark.MongoConnector.withCollectionDo(MongoConnector.scala:184)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1(MongoSpark.scala:118)\r\n\tat com.mongodb.spark.MongoSpark$.$anonfun$save$1$adapted(MongoSpark.scala:117)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1011)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1011)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2268)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "ml = spark.read.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_train\").load()\n",
    "train.write.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_train\").save()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a614d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_test = spark.read.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_test\").load()\n",
    "test.write.format(\"mongo\").option(\"uri\",\"mongodb://localhost:27017/walmart.data_test\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a1ddb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d612ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92c798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c497dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f544a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbaeefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',27017)\n",
    "db = client.walmart\n",
    "data = db.data_for_ml\n",
    "new_df = pd.DataFrame(list(data.find()))\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83105030",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data = new_df.drop(['_id'],axis=1)\n",
    "walmart_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ef275",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('localhost',27017)\n",
    "db_test = client.walmart\n",
    "data_test = db.data_for_ml_test\n",
    "new_test = pd.DataFrame(list(data.find()))\n",
    "new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_test = new_test.drop(['_id'],axis=1)\n",
    "walmart_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca2d332",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(walmart_data.info())\n",
    "print (\"*****************************************\")\n",
    "print(walmart_data_test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54abd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Numerical Data\n",
    "numeric_walmart_data=[key for key in dict(walmart_data.dtypes) if dict(walmart_data.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "walmart_data_num=walmart_data[numeric_walmart_data]\n",
    "print(walmart_data_num)\n",
    "# Train Categorical Data\n",
    "cat_walmart_data=[key for key in dict(walmart_data.dtypes) if dict(walmart_data.dtypes)[key] in ['object']]\n",
    "walmart_data_cat=walmart_data[cat_walmart_data]\n",
    "print(walmart_data_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f6366",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_num.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_cat.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1076f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Numerical Data\n",
    "numeric_walmart_data_test=[key for key in dict(walmart_data_test.dtypes) if dict(walmart_data_test.dtypes)[key] in ['float64', 'int64', 'float32', 'int32']]\n",
    "walmart_data_test_num=walmart_data_test[numeric_walmart_data_test]\n",
    "print(walmart_data_test_num)\n",
    "# Test Categorical Data\n",
    "cat_walmart_data_test=[key for key in dict(walmart_data_test.dtypes) if dict(walmart_data_test.dtypes)[key] in ['object']]\n",
    "walmart_data_test_cat=walmart_data_test[cat_walmart_data_test]\n",
    "print(walmart_data_test_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_test_num.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_test_cat.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fc20a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcfbc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd49c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62f8960a",
   "metadata": {},
   "source": [
    "# correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28041a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr=pd.DataFrame(walmart_data.corr())\n",
    "\n",
    "train_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12366009",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corr=pd.DataFrame(walmart_data_test.corr())\n",
    "#test_corr.to_excel(writer,'Test_Data Corr',index=True)\n",
    "test_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138fc1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(train_corr.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(test_corr.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaab5f",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec86d918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46407947",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(walmart_data.Weekly_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.plot(kind='line', x='Weekly_Sales', y='Store', alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbc8ba5",
   "metadata": {},
   "source": [
    "sales vs type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fea4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=walmart_data[\"Weekly_Sales\"],y=walmart_data[\"Type\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46953ec6",
   "metadata": {},
   "source": [
    "sales vs department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(walmart_data.isnull().sum())\n",
    "print(\"*\"*30)\n",
    "print(walmart_data_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abcb788",
   "metadata": {},
   "source": [
    "outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07053fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.Weekly_Sales=np.where(walmart_data.Weekly_Sales>100000, 100000,walmart_data.Weekly_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2232a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.Weekly_Sales.plot.hist(bins=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d639fa4",
   "metadata": {},
   "source": [
    "Feature Extraction\n",
    "\n",
    "In this section, we select the appropriate features to train our classifier. Here, we create new features based on existing features. We also convert categorical features into numeric form.\n",
    "\n",
    "Date Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad8821",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0a07be",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data['Date'] = pd.to_datetime(walmart_data['Date'])\n",
    "walmart_data_test['Date'] = pd.to_datetime(walmart_data_test['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0554079e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date features\n",
    "walmart_data['Date_dayofweek'] =walmart_data['Date'].dt.dayofweek\n",
    "walmart_data['Date_month'] =walmart_data['Date'].dt.month \n",
    "walmart_data['Date_year'] =walmart_data['Date'].dt.year\n",
    "walmart_data['Date_day'] =walmart_data['Date'].dt.day \n",
    "\n",
    "walmart_data_test['Date_dayofweek'] =walmart_data_test['Date'].dt.dayofweek\n",
    "walmart_data_test['Date_month'] =walmart_data_test['Date'].dt.month \n",
    "walmart_data_test['Date_year'] =walmart_data_test['Date'].dt.year\n",
    "walmart_data_test['Date_day'] =walmart_data_test['Date'].dt.day "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c1abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(walmart_data.Type.value_counts())\n",
    "print(\"*\"*30)\n",
    "print(walmart_data_test.Type.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd95358",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(walmart_data.IsHoliday.value_counts())\n",
    "print(\"*\"*30)\n",
    "print(walmart_data_test.IsHoliday.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee71a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [walmart_data, walmart_data_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df68651",
   "metadata": {},
   "source": [
    "Converting Categorical Variable 'Type' into Numerical Variable \n",
    "For A=1 , B=2, C=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {\"A\": 1, \"B\": 2, \"C\": 3}\n",
    "for dataset in train_test_data:\n",
    "    dataset['Type'] = dataset['Type'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336af9c9",
   "metadata": {},
   "source": [
    "Converting Categorical Variable 'IsHoliday' into Numerical Variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_mapping = {False: 0, True: 1}\n",
    "for dataset in train_test_data:\n",
    "    dataset['IsHoliday'] = dataset['IsHoliday'].map(type_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0328ada5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e20af5a",
   "metadata": {},
   "source": [
    "Creating Extra Holiday Variable.\n",
    "\n",
    "\n",
    "If that week comes under extra holiday then 1(=Yes) else 2(=No)\n",
    "\n",
    "\n",
    "\n",
    "Making New Holiday Variable Based on Given Data...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data['Super_Bowl'] = np.where((walmart_data['Date']==datetime(2010, 2, 12)) | (walmart_data['Date']==datetime(2011, 2, 11)) | (walmart_data['Date']==datetime(2012, 2, 10)) | (walmart_data['Date']==datetime(2013, 2, 8)),1,0)\n",
    "walmart_data['Labour_Day'] = np.where((walmart_data['Date']==datetime(2010, 9, 10)) | (walmart_data['Date']==datetime(2011, 9, 9)) | (walmart_data['Date']==datetime(2012, 9, 7)) | (['Date']==datetime(2013, 9, 6)),1,0)\n",
    "walmart_data['Thanksgiving'] = np.where((walmart_data['Date']==datetime(2010, 11, 26)) | (walmart_data['Date']==datetime(2011, 11, 25)) | (walmart_data['Date']==datetime(2012, 11, 23)) | (walmart_data['Date']==datetime(2013, 11, 29)),1,0)\n",
    "walmart_data['Christmas'] = np.where((walmart_data['Date']==datetime(2010, 12, 31)) | (walmart_data['Date']==datetime(2011, 12, 30)) | (walmart_data['Date']==datetime(2012, 12, 28)) | (walmart_data['Date']==datetime(2013, 12, 27)),1,0)\n",
    "#........................................................................\n",
    "walmart_data_test['Super_Bowl'] = np.where((walmart_data_test['Date']==datetime(2010, 2, 12)) | (walmart_data_test['Date']==datetime(2011, 2, 11)) | (walmart_data_test['Date']==datetime(2012, 2, 10)) | (walmart_data_test['Date']==datetime(2013, 2, 8)),1,0)\n",
    "walmart_data_test['Labour_Day'] = np.where((walmart_data_test['Date']==datetime(2010, 9, 10)) | (walmart_data_test['Date']==datetime(2011, 9, 9)) | (walmart_data_test['Date']==datetime(2012, 9, 7)) | (walmart_data_test['Date']==datetime(2013, 9, 6)),1,0)\n",
    "walmart_data_test['Thanksgiving'] = np.where((walmart_data_test['Date']==datetime(2010, 11, 26)) | (walmart_data_test['Date']==datetime(2011, 11, 25)) | (walmart_data_test['Date']==datetime(2012, 11, 23)) | (walmart_data_test['Date']==datetime(2013, 11, 29)),1,0)\n",
    "walmart_data_test['Christmas'] = np.where((walmart_data_test['Date']==datetime(2010, 12, 31)) | (walmart_data_test['Date']==datetime(2011, 12, 30)) | (walmart_data_test['Date']==datetime(2012, 12, 28)) | (walmart_data_test['Date']==datetime(2013, 12, 27)),1,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data['IsHoliday']=walmart_data['IsHoliday']|walmart_data['Super_Bowl']|walmart_data['Labour_Day']|walmart_data['Thanksgiving']|walmart_data['Christmas']\n",
    "walmart_data_test['IsHoliday']=walmart_data_test['IsHoliday']|walmart_data_test['Super_Bowl']|walmart_data_test['Labour_Day']|walmart_data_test['Thanksgiving']|walmart_data_test['Christmas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbb2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (walmart_data.Christmas.value_counts())\n",
    "print (walmart_data.Super_Bowl.value_counts())\n",
    "print (walmart_data.Thanksgiving.value_counts())\n",
    "print (walmart_data.Labour_Day.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2020f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (walmart_data_test.Christmas.value_counts())\n",
    "print (walmart_data_test.Super_Bowl.value_counts())\n",
    "print (walmart_data_test.Thanksgiving.value_counts())\n",
    "print (walmart_data_test.Labour_Day.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cbe98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have Imputed IsHoliday according to Extra holidays..These extra holiday variable has redundant..\n",
    "# Droping the Extra holiday variables because its redundant..\n",
    "dp=['Super_Bowl','Labour_Day','Thanksgiving','Christmas']\n",
    "walmart_data.drop(dp,axis=1,inplace=True)\n",
    "walmart_data_test.drop(dp,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfe2f01",
   "metadata": {},
   "source": [
    "Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782d6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4cb0e",
   "metadata": {},
   "source": [
    "Droping irrevelent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0f4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_drop=['CPI','Unemployment']\n",
    "walmart_data=walmart_data.drop(features_drop, axis=1)\n",
    "walmart_data_test=walmart_data_test.drop(features_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f27c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03790e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "walmart_data_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27513de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all float var int integer..\n",
    "#for var in walmart_data:\n",
    "    #if walmart_data[var].dtypes == float:\n",
    "       # walmart_data[var]=walmart_data[var].astype(int)\n",
    "        \n",
    "#for var in walmart_data_test:\n",
    "   # if walmart_data_test[var].dtypes == float:\n",
    "       # walmart_data_test[var]=walmart_data_test[var].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a365003",
   "metadata": {},
   "source": [
    "Classification & Accuracy\n",
    "\n",
    "\n",
    "Define training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755f2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train X= Exery thing except Weekly_Sales\n",
    "train_X=walmart_data.drop(['Weekly_Sales','Date'], axis=1)\n",
    "\n",
    "#### train Y= Only Weekly_Sales \n",
    "train_y=walmart_data['Weekly_Sales'] \n",
    "test_X=walmart_data_test.drop(['Date','Fuel_Price'],axis=1).copy()\n",
    "\n",
    "train_X.shape, train_y.shape, test_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2029ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.info()\n",
    "\n",
    "train_X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e5804",
   "metadata": {},
   "source": [
    "Building models & comparing their RMSE values\n",
    "\n",
    "\n",
    "1.Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bb5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d910a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Methood 1..\n",
    "clf = LinearRegression()\n",
    "clf.fit(train_X, train_y)\n",
    "y_pred_linear=clf.predict(test_X)\n",
    "acc_linear=round( clf.score(train_X, train_y) * 100, 2)\n",
    "print ('scorbe:'+str(acc_linear) + ' percent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee1f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
